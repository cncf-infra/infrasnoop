#+TITLE: Gathering Data for k8s-infra queries

* Goal
We want to be able to query the vast amount of data around prow jobs, so that we
can make targeted, smart efforts to change them upon request.

We have several data sources that we want to combine into a single postgres db
for querying.

This document outlines the steps needed to gather the data, along with the db
schemas for our postgres db. Eventually, the sql src in here will be lifted into
a manifest for a cluster pod. For now, it is recommended to run the src blocks
to create the files necessary, to be able to review whether the work is ready to be
converted into a job.

* Our database and workspace dir
We will be using a postgrest:14 database with the plpython3u extension. In the cluster-job
we are building for it, plpython3u is already included.  For this review, we will run it inside
docker.

#+NAME: Our docker image
#+begin_src docker :tangle ./Dockerfile :comments no
FROM postgres:14-bullseye
RUN apt-get update
RUN apt-get -y install python3 postgresql-plpython3-14
#+end_src

The cluster job also has access to a workspace directory.  Part of the job will be to pull
in files located in this dir to populate the tables.  For this review, we'll create a directory, mount
it in the docker container, and move all our neessary files to this workspace.

#+NAME: create dir
#+begin_src sh
mkdir ws
#+end_src

#+RESULTS: create dir
#+begin_src sh
#+end_src

After tangling the docker file, we can start our empty db.

#+begin_src tmate :window docker
docker build -t infradb:1 .
#+end_src

#+begin_src tmate :window docker
docker run --rm --name infradb \
    -e POSTGRES_PASSWORD=infradb \
    -p 5432:5432 \
    -v $(pwd)/ws:/workspace
    infradb:1
#+end_src


This will start a db with the following credentials:
- user :: postgres
- db :: postgres
- server :: localhost
- password :: infradb

With these, we should be able to connect to it through this org file.
We should expect no relations, a clean db.

#+NAME: database say hi
#+begin_src sql-mode
\
d
#+end_src

#+RESULTS: database say hi
#+begin_SRC example
Did not find any relations.
#+end_SRC

and lastly, we will create the plpython3u extension

#+begin_src sql-mode
create extension plpython3u;
#+end_src

#+RESULTS:
#+begin_SRC example
CREATE EXTENSION
#+end_SRC

* Get authenticated for data fetching
We will need to use bq and gsutil for half of the data fetching.
#+begin_src shell
bq version
gsutil --version
#+end_src

#+RESULTS:
#+begin_example
This is BigQuery CLI 2.0.74
gsutil version: 5.8
#+end_example

I have an ii service account that i am using to authenticate, by passing gsutil my keyfile.json

#+begin_example sh
gcloud auth activate-service-account --key-file ii-service-account.json
#+end_example

Then set up the right project and initialize bq

#+begin_src shell
gcloud config set project ii-coop
bq ls
#+end_src
#+RESULTS:
#+begin_src sh

Welcome to BigQuery! This script will walk you through the
process of initializing your .bigqueryrc configuration file.

First, we need to set up your credentials if they do not
already exist.

Setting project_id ii-coop as the default.

BigQuery configuration complete! Type "bq" to get started.

#+end_src

If this worked, I should be able to run a bq query on some public data

#+begin_src sh
bq query -q --nouse_cache --format prettyjson --nouse_legacy_sql \
    "select job from k8s-gubernator.build.all where result = 'SUCCESS' limit 3;"
#+end_src

#+RESULTS:
#+begin_src sh
[
  {
    "job": "ci-npd-test"
  },
  {
    "job": "ci-npd-test"
  },
  {
    "job": "ci-npd-test"
  }
]
#+end_src

Fantastic! We are good to go.
* The Plan
Our design for this is to get all the data we need into json loaded to the same directory, then copy the json into the right table in the db.  For our
pod this will be a number of init jobs that are run, outputting their results into a shared /workspace volume for our postgres init job to consume and populate the db.

For now, I can put them all into a tmp workspace directory

#+begin_src sh
mkdir ~/tmp/workspace
tree ~/tmp/workspace
#+end_src

#+RESULTS:
#+begin_src sh
/Users/workzach/tmp/workspace

0 directories, 0 files
#+end_src

* Get the latest successful jobs
** Fetch Data
#+NAME: fetch latest successful jobs
#+begin_src sh
bq query -q \
    --nouse_cache \
    --max_rows 99999999 \
    --format prettyjson \
    --nouse_legacy_sql \
    "select job,path,number,started
       from k8s-gubernator.build.all
      where result = 'SUCCESS'
     qualify ROW_NUMBER() OVER(PARTITION BY job ORDER BY number desc) = 1 ;" \
| jq -cr .[] > workspace/latest_successful_jobs.json
#+end_src

This should give us a file where each line is valid json
#+begin_src sh
head -3 workspace/latest_successful_jobs.json
#+end_src

#+RESULTS:
#+begin_src sh
{"job":"ci-build-and-push-k8s-at-golang-tip","number":"1508154169955979264","path":"gs://kubernetes-jenkins/logs/ci-build-and-push-k8s-at-golang-tip/1508154169955979264","started":"2022-03-27 18:51:13"}
{"job":"ci-cluster-api-provider-gcp-make-conformance-v1alpha3-k8s-ci-artifacts","number":"1380214112272781312","path":"gs://kubernetes-jenkins/logs/ci-cluster-api-provider-gcp-make-conformance-v1alpha3-k8s-ci-artifacts/1380214112272781312","started":"2021-04-08 17:42:29"}
{"job":"ci-cri-containerd-e2e-cos-gce-ingress","number":"1508137308472217600","path":"gs://kubernetes-jenkins/logs/ci-cri-containerd-e2e-cos-gce-ingress/1508137308472217600","started":"2022-03-27 17:42:12"}
#+end_src

** Load into postgres
I think it is best to make separate tables for each of our raw
data, and then combine them into a view.

#+NAME: Schema for latest successful jobs
#+begin_src sql-mode
create table successful_jobs(
job text,
path text,
number text,
started timestamp,
primary key (job, path)
);

comment on table successful_jobs "prow jobs that have finished successfully, according to bq";
comment on column successful_jobs.job is "the prow job name";
comment on column successful_jobs.number is "the number of this job run. Goes up incrementally";
comment on column successful_jobs.path is "where artifact will be stored. prowjob + number";
comment on column successful_jobs.started is "Date and Time this job was run";
#+end_src

#+RESULTS: Schema for latest successful jobs
#+begin_SRC example
CREATE TABLE
#+end_SRC

#+NAME: Load data into table
#+begin_src sql-mode
begin;
create temporary table success_import(data jsonb);
\copy success_import(data)  from 'ws/latest_successful_jobs.json' csv quote e'\x01' delimiter e'\x02';

insert into successful_jobs(job,path,number,started)
select  i.data->>'job',
        i.data->>'path',
        i.data->>'number',
        to_timestamp(i.data->>'started', 'YYYY-MM-DD HH24:MI:SS') as started
from success_import i;

commit;
#+end_src

#+RESULTS: Load data into table
#+begin_SRC example
BEGIN
postgres=*# CREATE TABLE
postgres=*# COPY 7514
postgres=*# postgres=*# postgres-*# postgres-*# postgres-*# postgres-*# postgres-*# INSERT 0 7514
postgres=*# postgres=*# COMMIT
#+end_SRC

Now we can query the jobs by timestamp.  One thing we will want is the # of jobs that have run this month.
we can query that as:
#+begin_src sql-mode
select count(*)
  from successful_jobs
 where started >= now() - interval '30 days';
#+end_src

#+RESULTS:
#+begin_SRC example
 count
-------
  1884
(1 row)

#+end_SRC

* TODO Get information about prow configs
* Get job paths and sizes
** Fetch Data
This is likely goihng to change, and we had to do a lot of terminal work to get the data exactly as we watned, but this is a good
scrap to remind us before we build a better verison
#+begin_example sh :dir ~/tmp/workspace/
cat latest_successful_jobs.json | jq -r '.path + "/**"' > gsutil_arguments.txt
cat ~/tmp/workspace/gsutil_arguments.txt | xargs -L 100 -P 16 gsutil ls -l > ~/tmp/workspace/job_logs.txt
cat job_logs.txt | grep -v "TOTAL:" | gsed "s/^ *//" | gsed "s/ *$//" | gsed "s/ \+/,/" | gsed "s/ \+/,/" > massaged.csv
#+end_example

#+RESULTS:
#+begin_src sh
#+end_src

This gets us a massaged.csv of size,time,path that we load into our db.
** Load into postgres

#+begin_src sql-mode
begin;
create table job_gcs_output(
size bigint,
time timestamp,
path text primary key
);

comment on table job_gcs_output is 'the path and size of resulting files from a test run';
comment on column job_gcs_output.size is 'size of the file in bytes';
comment on column job_gcs_output.time is 'time file was added';
comment on column job_gcs_output.path is 'full filepath';

create temporary table job_import(size text, time text, path text);

\copy job_import(size,time,path)  from '~/tmp/workspace/massaged.csv' csv;


insert into job_gcs_output(size,time,path)

select i.size::bigint as size,
      i.time::timestamp,
      i.path
 from job_import i;
commit;
#+end_src

#+RESULTS:
#+begin_SRC example
BEGIN
postgres=*# postgres(*# postgres(*# postgres(*# postgres(*# CREATE TABLE
postgres=*# postgres=*# COMMENT
postgres=*# COMMENT
postgres=*# COMMENT
postgres=*# COMMENT
postgres=*# postgres=*# CREATE TABLE
postgres=*# postgres=*# COPY 435919
postgres=*# postgres=*# postgres=*# postgres-*# postgres-*# postgres-*# postgres-*# postgres-*# INSERT 0 435919
postgres=*# COMMIT
#+end_SRC

Fantastic, now we can combine this with our successful jobs, to see the total artifact file size for the last 30 days' successful jobs

#+NAME: total file size of month's jobs
#+begin_src sql-mode
create materialized view recent_jobs as
  select s.job,
         s.number,
         s.started,
         j.path,
         j.size
select j.size , s.job
  from      job_gcs_output j
            join successful_jobs s on (j.path like '%'||s.path||'%')
 where s.started >= now() - interval '30 days'
       limit 10;
#+end_src

#+begin_src sql-mode
create materialized view recent_jobs as
select
  s.job,
  s.started,
  j.size,
  substring(substring(j.path, '\/[a-z\-_:\.\$ ()A-Z0-9]+$'),2) as file,
  j.path
  from      job_gcs_output j
            join successful_jobs s on (j.path like '%'||s.path||'%')
 where s.started >= now() - interval '30 days';
#+end_src

#+RESULTS:
#+begin_SRC example
SELECT 411133
#+end_SRC

#+begin_src sql-mode
select (sum(size)/1073741824) as size_in_gb -- there are 1,073,741,824 bytes in a gB
  from recent_jobs where file like '%json'
                      or file like '%log';
#+end_src

#+RESULTS:
#+begin_SRC example
      size_in_gb
----------------------
 113.2267135195434093
(1 row)

#+end_SRC

#+RESULTS: total file size of month's jobs
#+begin_SRC example
 size  |                    job
-------+-------------------------------------------
 97990 | e2e-kops-grid-cilium-amzn2-k21-containerd
  1681 | e2e-kops-grid-cilium-amzn2-k21-containerd
  9198 | e2e-kops-grid-cilium-amzn2-k21-containerd
 19161 | e2e-kops-grid-cilium-amzn2-k21-containerd
 13932 | e2e-kops-grid-cilium-amzn2-k21-containerd
  1595 | e2e-kops-grid-cilium-amzn2-k21-containerd
  8672 | e2e-kops-grid-cilium-amzn2-k21-containerd
 15136 | e2e-kops-grid-cilium-amzn2-k21-containerd
 98426 | e2e-kops-grid-cilium-amzn2-k21-containerd
  1595 | e2e-kops-grid-cilium-amzn2-k21-containerd
(10 rows)

#+end_SRC

* scratch
** STRT Query combined data
So now we can combine our latest_successful_jobs with this job_gcs_output to get info on specific jobsj
*** function: load containerd.json for job
We want to loop over our resulting files, fetch them from gcsweb.k8s.io and insert them into a files table.
that files table can have the job, the path, the started (for the job), the filename,

#+begin_src sql-mode
    begin;
create unlogged table file(
filename text,
job text,
started timestamp,
path text,
text_content text,
json jsonb
);
commit;
#+end_src

#+RESULTS:
#+begin_SRC example
BEGIN
postgres=*# postgres(*# postgres(*# postgres(*# postgres(*# postgres(*# postgres(*# postgres(*# CREATE TABLE
postgres=*# COMMIT
#+end_SRC

** Initial function
given a path, construct a publ;ic link from it, then return the value.

#+NAME: initial function
#+begin_src sql-mode
begin;

create function fetch_file(filepath text) returns text
as $$
    import urllib.request
    import hashlib

    workdir = '/workspace/'

    public_prefix = 'https://gcsweb.k8s.io/gcs/';
    path = filepath.replace("gs://","")
    link = public_prefix + path
    output = ""

    with urllib.request.urlopen(link) as response:

     # create a hash for the resulting filename
     hash_object = hashlib.sha256(filepath.encode('utf-8'))
     hex_dig = hash_object.hexdigest()
     output = workdir + str(hex_dig) + '.txt'

     # read the file
     file = str(response.read())
     f = open(output, 'w')
     f.write(file)
     f.close()
    return output
    end;
$$ language plpython3u;

commit;
#+end_src

#+RESULTS: initial function
#+begin_SRC example
BEGIN
postgres=*# postgres=*# postgres-*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# ERROR:  function "fetch_file" already exists with same argument types
postgres=!# postgres=!# ROLLBACK
#+end_SRC

With the fetch file, we can build out another function that inserts into our file table

#+begin_src sql-mode
\d+ recent_jobs;
#+end_src

#+RESULTS:
#+begin_SRC example
                                            Materialized view "public.recent_jobs"
 Column  |            Type             | Collation | Nullable | Default | Storage  | Compression | Stats target | Description
---------+-----------------------------+-----------+----------+---------+----------+-------------+--------------+-------------
 job     | text                        |           |          |         | extended |             |              |
 started | timestamp without time zone |           |          |         | plain    |             |              |
 size    | bigint                      |           |          |         | plain    |             |              |
 file    | text                        |           |          |         | extended |             |              |
 path    | text                        |           |          |         | extended |             |              |
View definition:
 SELECT s.job,
    s.started,
    j.size,
    "substring"("substring"(j.path, '\/[a-z\-_:\.\$ ()A-Z0-9]+$'::text), 2) AS file,
    j.path
   FROM job_gcs_output j
     JOIN successful_jobs s ON j.path ~~ (('%'::text || s.path) || '%'::text)
  WHERE s.started >= (now() - '30 days'::interval);
Access method: heap

#+end_SRC

#+NAME: insert_file fn
#+begin_src sql-mode
begin;
create procedure insert_file(filepath text)
as $$
  declare
  blob_path text;
  copy_command text;
begin

  blob_path = fetch_file(filepath);

  insert into file(filename,job,started,path)

  select j.file as filename,
         j.job,
         j.started,
         j.path
    from recent_jobs j
   where j.path = filepath;

  copy_command = 'copy file(text_content) from '''||blob_path||''' where file.path = '''||filepath||''';';
  execute copy_command;
end;
$$
language plpgsql;

commit;
#+end_src

#+RESULTS: insert_file fn
#+begin_SRC example
BEGIN
postgres=*# postgres-*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres-*# CREATE PROCEDURE
postgres=*# postgres=*# COMMIT
#+end_SRC

#+begin_src sql-mode
begin;

create procedure insert_files_for(filename text)
as $$
  declare
  table_record RECORD;
begin
  for table_record in
    select * from recent_jobs where recent_jobs.file like '%'||filename
    loop
    call insert_file(table_record.path);
    end loop;
end;
$$ language plpgsql;

call insert_files_for('images-containerd.log');
commit;
#+end_src
