#+TITLE: Gathering Data for k8s-infra queries

* Goal
We want to be able to query the vast amount of data around prow jobs, so that we
can make targeted, smart efforts to change them upon request.

We have several data sources that we want to combine into a single postgres db
for querying.

This document outlines the steps needed to gather the data, along with the db schemas
for our postgres db. The src blocks in this document can be lifted into a manifest for an
eventual infrasnoop pod.
* TODO Start up postgres DB
For this early iteration, I'll just use postgres in a docker container

#+NAME: Start up postgres on docker
#+begin_src tmate :window docker
docker run --rm --name infradb \
    -e POSTGRES_PASSWORD=infradb \
    -p 5432:5432 \
    postgres:14-bullseye
#+end_src


This will start a db with the following credentials:
- user :: postgres
- db :: postgres
- server :: localhost
- password :: infradb

With these, we should be able to connect to it through this org file.
We should expect no relations, a clean db.

#+NAME: connect to our db
#+begin_src sql-mode
\d
#+end_src

#+RESULTS: connect to our db
#+begin_SRC example
Did not find any relations.
#+end_SRC

* DONE Get authenticated for data fetching
We will need to use bq and gsutil for half of the data fetching.
#+begin_src shell
bq version
gsutil --version
#+end_src

#+RESULTS:
#+begin_example
This is BigQuery CLI 2.0.74
gsutil version: 5.8
#+end_example

I have an ii service account that i am using to authenticate, by passing gsutil my keyfile.json

#+begin_example sh
gcloud auth activate-service-account --key-file ii-service-account.json
#+end_example

Then set up the right project and initialize bq

#+begin_src shell
gcloud config set project ii-coop
bq ls
#+end_src
#+RESULTS:
#+begin_src sh

Welcome to BigQuery! This script will walk you through the
process of initializing your .bigqueryrc configuration file.

First, we need to set up your credentials if they do not
already exist.

Setting project_id ii-coop as the default.

BigQuery configuration complete! Type "bq" to get started.

#+end_src

If this worked, I should be able to run a bq query on some public data

#+begin_src sh
bq query -q --nouse_cache --format prettyjson --nouse_legacy_sql \
    "select job from k8s-gubernator.build.all where result = 'SUCCESS' limit 3;"
#+end_src

#+RESULTS:
#+begin_src sh
[
  {
    "job": "ci-npd-test"
  },
  {
    "job": "ci-npd-test"
  },
  {
    "job": "ci-npd-test"
  }
]
#+end_src

Fantastic! We are good to go.
* The Plan
Our design for this is to get all the data we need into json loaded to the same directory, then copy the json into the right table in the db.  For our
pod this will be a number of init jobs that are run, outputting their results into a shared /workspace volume for our postgres init job to consume and populate the db.

For now, I can put them all into a tmp workspace directory

#+begin_src sh
mkdir ~/tmp/workspace
tree ~/tmp/workspace
#+end_src

#+RESULTS:
#+begin_src sh
/Users/workzach/tmp/workspace

0 directories, 0 files
#+end_src

* DONE [2/2] Get the latest successful jobs
** DONE Fetch Data
#+NAME: fetch latest successful jobs
#+begin_src sh
bq query -q \
    --nouse_cache \
    --max_rows 99999999 \
    --format prettyjson \
    --nouse_legacy_sql \
    "select job,path,number,started
       from k8s-gubernator.build.all
      where result = 'SUCCESS'
     qualify ROW_NUMBER() OVER(PARTITION BY job ORDER BY number desc) = 1 ;" \
| jq -cr .[] > ~/tmp/workspace/latest_successful_jobs.json
#+end_src

This should give us a file where each line is valid json
#+begin_src sh
head -3 ~/tmp/workspace/latest_successful_jobs.json
#+end_src

#+RESULTS:
#+begin_src sh
{"job":"ci-build-and-push-k8s-at-golang-tip","number":"1508154169955979264","path":"gs://kubernetes-jenkins/logs/ci-build-and-push-k8s-at-golang-tip/1508154169955979264","started":"2022-03-27 18:51:13"}
{"job":"ci-cluster-api-provider-gcp-make-conformance-v1alpha3-k8s-ci-artifacts","number":"1380214112272781312","path":"gs://kubernetes-jenkins/logs/ci-cluster-api-provider-gcp-make-conformance-v1alpha3-k8s-ci-artifacts/1380214112272781312","started":"2021-04-08 17:42:29"}
{"job":"ci-cri-containerd-e2e-cos-gce-ingress","number":"1508137308472217600","path":"gs://kubernetes-jenkins/logs/ci-cri-containerd-e2e-cos-gce-ingress/1508137308472217600","started":"2022-03-27 17:42:12"}
#+end_src

** DONE Load into postgres
I think it is best to make separate tables for each of our raw
data, and then combine them into a view.

#+begin_src sql-mode
drop table successful_jobs;
#+end_src

#+RESULTS:
#+begin_SRC example
DROP TABLE
#+end_SRC

#+NAME: Schema for latest successful jobs
#+begin_src sql-mode
create table successful_jobs(
job text,
path text,
number text,
started timestamp,
primary key (job, path)
);
#+end_src

#+RESULTS: Schema for latest successful jobs
#+begin_SRC example
CREATE TABLE
#+end_SRC

#+NAME: Load data into table
#+begin_src sql-mode
begin;
create temporary table success_import(data jsonb);
\copy success_import(data)  from '~/tmp/workspace/latest_successful_jobs.json' csv quote e'\x01' delimiter e'\x02';

insert into successful_jobs(job,path,number,started)
select  i.data->>'job',
        i.data->>'path',
        i.data->>'number',
        to_timestamp(i.data->>'started', 'YYYY-MM-DD HH24:MI:SS') as started
from success_import i;

commit;
#+end_src

#+RESULTS: Load data into table
#+begin_SRC example
BEGIN
postgres=*# CREATE TABLE
postgres=*# COPY 7514
postgres=*# postgres=*# postgres-*# postgres-*# postgres-*# postgres-*# postgres-*# INSERT 0 7514
postgres=*# postgres=*# COMMIT
#+end_SRC

Now we can query the jobs by timestamp, like the last
10 jobs that ran in the last 12 hours, ordered by earliest
#+begin_src sql-mode
select job, path, started
from successful_jobs
where number is null
order by started asc
limit 10;
#+end_src

#+RESULTS:
#+begin_SRC example
                               job                               |                                                               path                                                               |       started
-----------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------+---------------------
 pull-kubernetes-verify                                          | gs://kubernetes-jenkins/logs/pull-kubernetes-verify/20161016-0756271e80eef7b01f7529-3685                                         | 2016-10-16 07:56:27
 ci-kubernetes-e2e-gce-ubuntu-node-prow                          | gs://kubernetes-jenkins/logs/ci-kubernetes-e2e-gce-ubuntu-node-prow/20170802-211931--116b094ba9c718b6-7                          | 2017-08-02 21:19:31
 ci-kubernetes-e2enode-cosstable1-k8sstable1-default-defaultspec | gs://kubernetes-jenkins/logs/ci-kubernetes-e2enode-cosstable1-k8sstable1-default-defaultspec/20170807-204445--18a650865f3197bb-7 | 2017-08-07 20:44:45
 pr:632                                                          | gs://kubernetes-jenkins/pr-logs/pull/kubeflow_testing/632/kubeflow-testing-presubmit                                             | 2020-03-26 00:48:29
(4 rows)

#+end_SRC

Sweet, onto the next!

* TODO Get information about prow configs
* DONE Get job paths and sizes
** Fetch Data
This is likely goihng to change, and we had to do a lot of terminal work to get the data exactly as we watned, but this is a good
scrap to remind us before we build a better verison
#+begin_example sh :dir ~/tmp/workspace/
cat latest_successful_jobs.json | jq -r '.path + "/**"' > gsutil_arguments.txt
cat ~/tmp/workspace/gsutil_arguments.txt | xargs -L 100 -P 16 gsutil ls -l > ~/tmp/workspace/job_logs.txt
cat job_logs.txt | grep -v "TOTAL:" | gsed "s/^ *//" | gsed "s/ *$//" | gsed "s/ \+/,/" | gsed "s/ \+/,/" > massaged.csv
#+end_example

#+RESULTS:
#+begin_src sh
#+end_src

This gets us a massaged.csv of size,time,path that we load into our db.
** Load into postgres

#+begin_src sql-mode
begin;
create table job_gcs_output(
size bigint,
time timestamp,
path text primary key
);

create temporary table job_import(size text, time text, path text);

\copy job_import(size,time,path)  from '~/tmp/workspace/massaged.csv' csv;


insert into job_gcs_output(size,time,path)

select i.size::bigint as size,
      i.time::timestamp,
      i.path
 from job_import i;
commit;
#+end_src

#+RESULTS:
#+begin_SRC example
BEGIN
postgres=*# postgres(*# postgres(*# postgres(*# postgres(*# CREATE TABLE
postgres=*# postgres=*# CREATE TABLE
postgres=*# postgres=*# COPY 435919
postgres=*# postgres=*# postgres=*# postgres-*# postgres-*# postgres-*# postgres-*# postgres-*# INSERT 0 435919
postgres=*# COMMIT
#+end_SRC

* STRT Query combined data
So now we can combine our latest_successful_jobs with this job_gcs_output to get info on specific jobsj
** jobs with images-containerd.log in output
We want to find jobs whose output contains images-containerd.log...with the end goal to be to load up this log into the db.

First, let's see if we can filter on this output
#+begin_src sql-mode
    select count(distinct path)
    from job_gcs_output
    where path like '%images-containerd.log'
    limit 5;
#+end_src

#+RESULTS:
#+begin_SRC example
 count
-------
   351
(1 row)

#+end_SRC

Great, now we want to combine this with our successful jobs, to get the job name.


#+begin_src sql-mode
select s.job, j.path
from
job_gcs_output j
join successful_jobs s on j.path like '%'||s.path||'%'
where j.path like '%images-containerd.log'
limit 5;
#+end_src

#+RESULTS:
#+begin_SRC example
                  job                  |                                                                          path
---------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------
 ci-cri-containerd-e2e-cos-gce-ingress | gs://kubernetes-jenkins/logs/ci-cri-containerd-e2e-cos-gce-ingress/1508137308472217600/artifacts/bootstrap-e2e-master/images-containerd.log
 ci-cri-containerd-e2e-cos-gce-ingress | gs://kubernetes-jenkins/logs/ci-cri-containerd-e2e-cos-gce-ingress/1508137308472217600/artifacts/bootstrap-e2e-minion-group-jcm7/images-containerd.log
 ci-cri-containerd-e2e-cos-gce-ingress | gs://kubernetes-jenkins/logs/ci-cri-containerd-e2e-cos-gce-ingress/1508137308472217600/artifacts/bootstrap-e2e-minion-group-p8cd/images-containerd.log
 ci-cri-containerd-e2e-cos-gce-ingress | gs://kubernetes-jenkins/logs/ci-cri-containerd-e2e-cos-gce-ingress/1508137308472217600/artifacts/bootstrap-e2e-minion-group-tq61/images-containerd.log
 pr:pull-kubernetes-e2e-gci-gce-ipvs   | gs://kubernetes-jenkins/pr-logs/pull/109060/pull-kubernetes-e2e-gci-gce-ipvs/1508131198545694720/artifacts/bootstrap-e2e-master/images-containerd.log
(5 rows)

#+end_SRC

There seems to be multiple entries for each of these, as there are multiple artifacts that contain an images-containerd.log.

What we want to do now is, for each of these rows, fetch the log to put into a log table, where we can parse through the json as needed.
** function: load containerd.json for job
We want to loop over our resulting files, fetch them from gcsweb.k8s.io and insert them into a files table.
that files table can have the job, the path, the started (for the job), the filename,
#+begin_src sql-mode
    begin;
create table file(
filename text,
job text, --references successful_jobs(job),
started timestamp, --references successful_jobs(started),
path text references job_gcs_output(path), --references job_gcs_output(path),
text_content text,
json jsonb
);
commit;
#+end_src

#+RESULTS:
#+begin_SRC example
BEGIN
postgres=*# postgres(*# postgres(*# postgres(*# postgres(*# postgres(*# postgres(*# postgres(*# CREATE TABLE
postgres=*# COMMIT
#+end_SRC

* Initial function
given a path, construct a publ;ic link from it, then return the value.

#+NAME: initial function
#+begin_src sql-mode
begin;

create function fetch_file(filepath text) returns text
as $$
    import urllib.request

    public_prefix = 'https://gcsweb.k8s.io/gcs/';
    path = filepath.replace("gs://","")
    link = public_prefix + path
    file = ""

    with urllib.request.urlopen(link) as response:
     file = response.read()
    return file
    end;
$$ language plpython3u;

commit;
#+end_src

#+RESULTS: initial function
#+begin_SRC example
BEGIN
postgres=*# postgres=*# postgres-*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# CREATE FUNCTION
postgres=*# postgres=*# COMMIT
#+end_SRC

With the fetch file, we can build out another function that inserts into our file table

#+begin_src sql-mode
begin;
create function insert_file(filename text, filepath text) returns text
as $$
  declare
  p text;
begin
  -- insert into file(filename, job, started, path)

  -- select filename,
  --        s.job,
  --        s.started,
  --        j.path
  --   from job_gcs_output j
  --        join successful_jobs s on (j.path like '%'||s.path||'%')
  --  where j.path = filepath;

  -- blob = fetch_file(f)

  -- if f like '%.json' then
  --   update file
  --   set json = to_json(blob)
  --   where file.path
  -- else
  --   update

  select filepath into p;
   --  from job_gcs_output j
   -- where j.path = filepath
   -- limit 1;
  return p;
end;
$$
language plpgsql;

select * from insert_file('images-containerd.log',
                          ' gs://kubernetes-jenkins/logs/ci-kubernetes-e2e-gce-coredns-performance/1508096287960993792/artifacts/gce-coredns-perf-master/images-containerd.log');
rollback;
#+end_src

#+RESULTS:
#+begin_SRC example
BEGIN
postgres=*# postgres-*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres-*# CREATE FUNCTION
postgres=*# postgres=*# postgres(*#                                                                      insert_file
------------------------------------------------------------------------------------------------------------------------------------------------------
  gs://kubernetes-jenkins/logs/ci-kubernetes-e2e-gce-coredns-performance/1508096287960993792/artifacts/gce-coredns-perf-master/images-containerd.log
(1 row)

postgres=*# ROLLBACK
#+end_SRC
* misc
* Reconfigure ORG with k8s-cluster PGUSER+PGPASSWORD

Run this after you deploy

#+begin_src emacs-lisp :noweb yes :results silent
(setenv "PGUSER" (shell-command-to-string "kubectl -n infrasnoop get secrets k8s-infra.k8s-infra.credentials.postgresql.acid.zalan.do -o go-template='{{ .data.username | base64decode }}'"))
(setenv "PGPASSWORD" (shell-command-to-string "kubectl -n infrasnoop get secrets k8s-infra.k8s-infra.credentials.postgresql.acid.zalan.do -o go-template='{{ .data.password | base64decode }}'"))
(reconfigure-org)
#+end_src
