#+TITLE: Gathering Data for k8s-infra queries

* Goal
We want to be able to query the vast amount of data around prow jobs, so that we
can make targeted, smart efforts to change them upon request.

We have several data sources that we want to combine into a single postgres db
for querying.

This document outlines the steps needed to gather the data, along with the db schemas
for our postgres db. The src blocks in this document can be lifted into a manifest for an
eventual infrasnoop pod.
* TODO Start up postgres DB
For this early iteration, I'll just use postgres in a docker container

#+NAME: Start up postgres on docker
#+begin_src tmate :window docker
docker run --rm --name infradb \
    -e POSTGRES_PASSWORD=infradb \
    -p 5432:5432 \
    postgres:14-bullseye
#+end_src


This will start a db with the following credentials:
- user :: postgres
- db :: postgres
- server :: localhost
- password :: infradb

With these, we should be able to connect to it through this org file.
We should expect no relations, a clean db.

#+NAME: connect to our db
#+begin_src sql-mode
\d
#+end_src

#+RESULTS: connect to our db
#+begin_SRC example
Did not find any relations.
#+end_SRC

* DONE Get authenticated for data fetching
We will need to use bq and gsutil for half of the data fetching.
#+begin_src shell
bq version
gsutil --version
#+end_src

#+RESULTS:
#+begin_example
This is BigQuery CLI 2.0.74
gsutil version: 5.8
#+end_example

I have an ii service account that i am using to authenticate, by passing gsutil my keyfile.json

#+begin_example sh
gcloud auth activate-service-account --key-file ii-service-account.json
#+end_example

Then set up the right project and initialize bq

#+begin_src shell
gcloud config set project ii-coop
bq ls
#+end_src
#+RESULTS:
#+begin_src sh

Welcome to BigQuery! This script will walk you through the
process of initializing your .bigqueryrc configuration file.

First, we need to set up your credentials if they do not
already exist.

Setting project_id ii-coop as the default.

BigQuery configuration complete! Type "bq" to get started.

#+end_src

If this worked, I should be able to run a bq query on some public data

#+begin_src sh
bq query -q --nouse_cache --format prettyjson --nouse_legacy_sql \
    "select job from k8s-gubernator.build.all where result = 'SUCCESS' limit 3;"
#+end_src

#+RESULTS:
#+begin_src sh
[
  {
    "job": "ci-npd-test"
  },
  {
    "job": "ci-npd-test"
  },
  {
    "job": "ci-npd-test"
  }
]
#+end_src

Fantastic! We are good to go.
* The Plan
Our design for this is to get all the data we need into json loaded to the same directory, then copy the json into the right table in the db.  For our
pod this will be a number of init jobs that are run, outputting their results into a shared /workspace volume for our postgres init job to consume and populate the db.

For now, I can put them all into a tmp workspace directory

#+begin_src sh
mkdir ~/tmp/workspace
tree ~/tmp/workspace
#+end_src

#+RESULTS:
#+begin_src sh
/Users/workzach/tmp/workspace

0 directories, 0 files
#+end_src

* DONE [2/2] Get the latest successful jobs
** DONE Fetch Data
#+NAME: fetch latest successful jobs
#+begin_src sh
bq query -q \
    --nouse_cache \
    --max_rows 99999999 \
    --format prettyjson \
    --nouse_legacy_sql \
    "select job,path,number,started
       from k8s-gubernator.build.all
      where result = 'SUCCESS'
     qualify ROW_NUMBER() OVER(PARTITION BY job ORDER BY number desc) = 1 ;" \
| jq -cr .[] > ~/tmp/workspace/latest_successful_jobs.json
#+end_src

This should give us a file where each line is valid json
#+begin_src sh
head -3 ~/tmp/workspace/latest_successful_jobs.json
#+end_src

#+RESULTS:
#+begin_src sh
{"job":"ci-build-and-push-k8s-at-golang-tip","number":"1508154169955979264","path":"gs://kubernetes-jenkins/logs/ci-build-and-push-k8s-at-golang-tip/1508154169955979264","started":"2022-03-27 18:51:13"}
{"job":"ci-cluster-api-provider-gcp-make-conformance-v1alpha3-k8s-ci-artifacts","number":"1380214112272781312","path":"gs://kubernetes-jenkins/logs/ci-cluster-api-provider-gcp-make-conformance-v1alpha3-k8s-ci-artifacts/1380214112272781312","started":"2021-04-08 17:42:29"}
{"job":"ci-cri-containerd-e2e-cos-gce-ingress","number":"1508137308472217600","path":"gs://kubernetes-jenkins/logs/ci-cri-containerd-e2e-cos-gce-ingress/1508137308472217600","started":"2022-03-27 17:42:12"}
#+end_src

** DONE Load into postgres
I think it is best to make separate tables for each of our raw
data, and then combine them into a view.

#+NAME: Schema for latest successful jobs
#+begin_src sql-mode
create table successful_jobs(
job text,
path text,
number text,
started timestamp
);
#+end_src

#+RESULTS: Schema for latest successful jobs
#+begin_SRC example
CREATE TABLE
#+end_SRC

#+NAME: Load data into table
#+begin_src sql-mode
begin;
create temporary table success_import(data jsonb);
\copy success_import(data)  from '~/tmp/workspace/latest_successful_jobs.json' csv quote e'\x01' delimiter e'\x02';

insert into successful_jobs(job,path,number,started)
select  i.data->>'job',
        i.data->>'path',
        i.data->>'number',
        to_timestamp(i.data->>'started', 'YYYY-MM-DD HH24:MI:SS') as started
from success_import i;

commit;
#+end_src

#+RESULTS: Load data into table
#+begin_SRC example
BEGIN
postgres=*# CREATE TABLE
postgres=*# COPY 7514
postgres=*# postgres=*# postgres-*# postgres-*# postgres-*# postgres-*# postgres-*# INSERT 0 7514
postgres=*# postgres=*# COMMIT
#+end_SRC

Now we can query the jobs by timestamp, like the last
10 jobs that ran in the last 12 hours, ordered by earliest
#+begin_src sql-mode
select now() as now, job, number, started
from successful_jobs
where started >= current_timestamp - interval '12 hours'
order by started asc
limit 10;
#+end_src

#+RESULTS:
#+begin_SRC example
              now              |                              job                              |       number        |       started
-------------------------------+---------------------------------------------------------------+---------------------+---------------------
 2022-03-27 21:47:17.089326+00 | periodic-cluster-api-provider-vsphere-upgrade-main            | 1508019280833155072 | 2022-03-27 09:53:16
 2022-03-27 21:47:17.089326+00 | e2e-kops-grid-cilium-u2004-k23-docker                         | 1508019280891875328 | 2022-03-27 09:53:45
 2022-03-27 21:47:17.089326+00 | periodic-cluster-api-e2e-workload-upgrade-1-21-1-22-main      | 1508020286358818816 | 2022-03-27 09:59:07
 2022-03-27 21:47:17.089326+00 | e2e-kops-grid-cilium-u2004-k22-containerd                     | 1508021544805208064 | 2022-03-27 10:02:48
 2022-03-27 21:47:17.089326+00 | ci-kubernetes-e2e-gce-network-metric-measurement              | 1508024564645367808 | 2022-03-27 10:14:12
 2022-03-27 21:47:17.089326+00 | e2e-kops-grid-cilium-etcd-u2004-k22-docker                    | 1508025320211484672 | 2022-03-27 10:17:08
 2022-03-27 21:47:17.089326+00 | ci-cos-cgroupv2-containerd-node-e2e                           | 1508030856273334272 | 2022-03-27 10:39:12
 2022-03-27 21:47:17.089326+00 | periodic-cluster-api-provider-digitalocean-janitor            | 1508030856193642496 | 2022-03-27 10:39:15
 2022-03-27 21:47:17.089326+00 | periodic-cluster-api-provider-aws-e2e-conformance-release-0-7 | 1508030856164282368 | 2022-03-27 10:39:17
 2022-03-27 21:47:17.089326+00 | ci-kubernetes-e2e-kubeadm-kinder-external-etcd-1-23           | 1508031863669657600 | 2022-03-27 10:45:11
(10 rows)

#+end_SRC

Sweet, onto the next!

* TODO Get information about prow configs
* DONE Get job paths and sizes
** Fetch Data
This is likely goihng to change, and we had to do a lot of terminal work to get the data exactly as we watned, but this is a good
scrap to remind us before we build a better verison
#+begin_example sh :dir ~/tmp/workspace/
cat latest_successful_jobs.json | jq -r '.path + "/**"' > gsutil_arguments.txt
cat ~/tmp/workspace/gsutil_arguments.txt | xargs -L 100 -P 16 gsutil ls -l > ~/tmp/workspace/job_logs.txt
cat job_logs.txt | grep -v "TOTAL:" | gsed "s/^ *//" | gsed "s/ *$//" | gsed "s/ \+/,/" | gsed "s/ \+/,/" > massaged.csv
#+end_example

#+RESULTS:
#+begin_src sh
#+end_src

This gets us a massaged.csv of size,time,path that we load into our db.
** Load into postgres
#+begin_src sh :dir ~/tmp/workspace
#+end_src

#+RESULTS:
#+begin_src sh
#+end_src

#+begin_src sql-mode
begin;
create table job_gcs_output(
size bigint,
time timestamp,
path text
);

create temporary table job_import(size text, time text, path text);

\copy job_import(size,time,path)  from '~/tmp/workspace/massaged.csv' csv;


insert into job_gcs_output(size,time,path)

select i.size::bigint as size,
      i.time::timestamp,
      i.path
 from job_import i;
commit;
#+end_src

#+RESULTS:
#+begin_SRC example
BEGIN
postgres=*# postgres(*# postgres(*# postgres(*# postgres(*# CREATE TABLE
postgres=*# postgres=*# CREATE TABLE
postgres=*# postgres=*# COPY 435919
postgres=*# postgres=*# postgres=*# postgres-*# postgres-*# postgres-*# postgres-*# postgres-*# INSERT 0 435919
postgres=*# COMMIT
#+end_SRC

* STRT Query combined data
So now we can combine our latest_successful_jobs with this job_gcs_output to get info on specific jobsj
** jobs with images-containerd.log in output
We want to find jobs whose output contains images-containerd.log...with the end goal to be to load up this log into the db.

First, let's see if we can filter on this output
#+begin_src sql-mode
    select count(distinct path)
    from job_gcs_output
    where path like '%images-containerd.log'
    limit 5;
#+end_src

#+RESULTS:
#+begin_SRC example
 count
-------
   351
(1 row)

#+end_SRC

Great, now we want to combine this with our successful jobs, to get the job name.


#+begin_src sql-mode
select s.job, j.path
from
job_gcs_output j
join successful_jobs s on j.path like '%'||s.path||'%'
where j.path like '%images-containerd.log'
limit 5;
#+end_src

#+RESULTS:
#+begin_SRC example
                    job                    |                                                                        path
-------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------
 ci-kubernetes-e2e-gce-coredns-performance | gs://kubernetes-jenkins/logs/ci-kubernetes-e2e-gce-coredns-performance/1508096287960993792/artifacts/gce-coredns-perf-master/images-containerd.log
 ci-kubernetes-e2e-gci-gce-sig-cli         | gs://kubernetes-jenkins/logs/ci-kubernetes-e2e-gci-gce-sig-cli/1508074141863907328/artifacts/bootstrap-e2e-master/images-containerd.log
 ci-kubernetes-e2e-gci-gce-sig-cli         | gs://kubernetes-jenkins/logs/ci-kubernetes-e2e-gci-gce-sig-cli/1508074141863907328/artifacts/bootstrap-e2e-minion-group-2ppq/images-containerd.log
 ci-kubernetes-e2e-gci-gce-sig-cli         | gs://kubernetes-jenkins/logs/ci-kubernetes-e2e-gci-gce-sig-cli/1508074141863907328/artifacts/bootstrap-e2e-minion-group-cjx3/images-containerd.log
 ci-kubernetes-e2e-gci-gce-sig-cli         | gs://kubernetes-jenkins/logs/ci-kubernetes-e2e-gci-gce-sig-cli/1508074141863907328/artifacts/bootstrap-e2e-minion-group-m53c/images-containerd.log
(5 rows)

#+end_SRC

There seems to be multiple entries for each of these, as there are multiple artifacts that contain an images-containerd.log.

What we want to do now is, for each of these rows, fetch the log to put into a log table, where we can parse through the json as needed.
** function: load containerd.json for job
We want to loop over our resulting files, fetch them from gcsweb.k8s.io and insert them into a files table.
that files table can have the job, the path, the started (for the job), the filename,
#+begin_src sql-mode
create table file(
filename text,
job text, --references successful_jobs(job),
started timestamp, --references successful_jobs(started),
path text, --references job_gcs_output(path),
text_content text,
json jsonb
);
#+end_src


I believe next we can loop over a select statemetn and for each row...do stuff.

We will need to add a siongle if then, if the filename ends in json, put it in
json column, else put it in text column.

I think we will want to clean up the initial tables a bit, to have contraints on
the relations between tables, so that we don't have any duplication.

I think the ultimate signature would be

upsert_files('filename.json')
and it will select for all the files that match, then curl it down, and upsert it into table.
