#+TITLE: Gathering Data for k8s-infra queries

* Goal
We want to be able to query the vast amount of data around prow jobs, so that we
can make targeted, smart efforts to change them upon request.

We have several data sources that we want to combine into a single postgres db
for querying.

This document outlines the steps needed to gather the data, along with the db
schemas for our postgres db. Eventually, the sql src in here will be lifted into
a manifest for a cluster pod. For now, it is recommended to run the src blocks
to create the files necessary, to be able to review whether the work is ready to be
converted into a job.

* Our database and workspace dir
We will be using a postgrest:14 database with the plpython3u extension. In the cluster-job
we are building for it, plpython3u is already included.  For this review, we will run it inside
docker.

#+NAME: Our docker image
#+begin_src docker :tangle ./Dockerfile :comments no
FROM postgres:14-bullseye
RUN apt-get update
RUN apt-get -y install python3 postgresql-plpython3-14
#+end_src

The cluster job also has access to a workspace directory.  Part of the job will be to pull
in files located in this dir to populate the tables.  For this review, we'll create a directory, mount
it in the docker container, and move all our neessary files to this workspace.

#+NAME: create dir
#+begin_src sh :results silent
mkdir ws
#+end_src

After tangling the docker file, we can start our empty db.

#+begin_src tmate :window docker
docker build -t infradb:1 .
#+end_src

#+begin_src tmate :window docker
docker run --rm --name infradb \
    -e POSTGRES_PASSWORD=infradb \
    -p 5432:5432 \
    -v $(pwd)/ws:/workspace \
    infradb:1
#+end_src


This will start a db with the following credentials:
- user :: postgres
- db :: postgres
- server :: localhost
- password :: infradb

With these, we should be able to connect to it through this org file.
We should expect no relations, a clean db.

#+NAME: database say hi
#+begin_src sql-mode
\d
#+end_src

#+RESULTS: database say hi
#+begin_SRC example
Did not find any relations.
#+end_SRC

and lastly, we will create the plpython3u extension

#+begin_src sql-mode
create extension plpython3u;
#+end_src

#+RESULTS:
#+begin_SRC example
CREATE EXTENSION
#+end_SRC

* Get authenticated for data fetching
We will need to use bq and gsutil for half of the data fetching.
#+begin_src shell
bq version
gsutil --version
#+end_src

#+RESULTS:
#+begin_example
This is BigQuery CLI 2.0.74
gsutil version: 5.8
#+end_example

I have an ii service account that i am using to authenticate, by passing gsutil my keyfile.json

#+begin_example sh
gcloud auth activate-service-account --key-file ii-service-account.json
#+end_example

Then set up the right project and initialize bq

#+begin_src shell
gcloud config set project ii-coop
bq ls
#+end_src
#+RESULTS:
#+begin_src sh

Welcome to BigQuery! This script will walk you through the
process of initializing your .bigqueryrc configuration file.

First, we need to set up your credentials if they do not
already exist.

Setting project_id ii-coop as the default.

BigQuery configuration complete! Type "bq" to get started.

#+end_src

If this worked, I should be able to run a bq query on some public data

#+begin_src sh
bq query -q --nouse_cache --format prettyjson --nouse_legacy_sql \
    "select job from k8s-gubernator.build.all where result = 'SUCCESS' limit 3;"
#+end_src

#+RESULTS:
#+begin_src sh
[
  {
    "job": "ci-npd-test"
  },
  {
    "job": "ci-npd-test"
  },
  {
    "job": "ci-npd-test"
  }
]
#+end_src

Fantastic! We are good to go.
* Pull in Data sources
** Successful jobs
#+NAME: fetch latest successful jobs
#+begin_src sh
bq query -q \
    --nouse_cache \
    --max_rows 99999999 \
    --format prettyjson \
    --nouse_legacy_sql \
    "select job,path,number,started
       from k8s-gubernator.build.all
      where result = 'SUCCESS'
     qualify ROW_NUMBER() OVER(PARTITION BY job ORDER BY number desc) = 1 ;" \
| jq -cr .[] > workspace/latest_successful_jobs.json
#+end_src

This should give us a file where each line is valid json
#+begin_src sh
head -3 workspace/latest_successful_jobs.json
#+end_src

#+RESULTS:
#+begin_src sh
{"job":"ci-build-and-push-k8s-at-golang-tip","number":"1508154169955979264","path":"gs://kubernetes-jenkins/logs/ci-build-and-push-k8s-at-golang-tip/1508154169955979264","started":"2022-03-27 18:51:13"}
{"job":"ci-cluster-api-provider-gcp-make-conformance-v1alpha3-k8s-ci-artifacts","number":"1380214112272781312","path":"gs://kubernetes-jenkins/logs/ci-cluster-api-provider-gcp-make-conformance-v1alpha3-k8s-ci-artifacts/1380214112272781312","started":"2021-04-08 17:42:29"}
{"job":"ci-cri-containerd-e2e-cos-gce-ingress","number":"1508137308472217600","path":"gs://kubernetes-jenkins/logs/ci-cri-containerd-e2e-cos-gce-ingress/1508137308472217600","started":"2022-03-27 17:42:12"}
#+end_src

** Job artifacts and sizes
This is likely goihng to change, and we had to do a lot of terminal work to get the data exactly as we watned, but this is a good
scrap to remind us before we build a better verison
#+begin_example sh :dir ~/tmp/workspace/
cat latest_successful_jobs.json | jq -r '.path + "/**"' > gsutil_arguments.txt
cat ~/tmp/workspace/gsutil_arguments.txt | xargs -L 100 -P 16 gsutil ls -l > ~/tmp/workspace/job_logs.txt
cat job_logs.txt | grep -v "TOTAL:" | gsed "s/^ *//" | gsed "s/ *$//" | gsed "s/ \+/,/" | gsed "s/ \+/,/" > massaged.csv
#+end_example

#+RESULTS:
#+begin_src sh
#+end_src

This gets us a massaged.csv of size,time,path that we load into our db.

** information about prow configs
* Postgres tables and views
From our bq query, a table for all the prow jobs with a SUCESS result, along with their number, path, and when they started.
The path is the root for all our artifact paths later on, but i am unfortunately uncertain how to match them one to one.
** successful jobs
#+NAME: Schema for latest successful jobs
#+begin_src sql-mode :results silent
create table successful_jobs(
id bigint primary key generated by default as identity,
job text,
path text,
number text,
started timestamp
);

comment on table successful_jobs 'prow jobs that have finished successfully, according to bq';
comment on column successful_jobs.id is 'generated automatically, unique id for row';
comment on column successful_jobs.job is 'the prow job name';
comment on column successful_jobs.number is 'the number of this job run. Goes up incrementally';
comment on column successful_jobs.path is 'where artifact will be stored. prowjob + number';
comment on column successful_jobs.started is 'Date and Time this job was run';

create unique index job_path_idx on successful_jobs(path);
#+end_src

#+RESULTS:
#+begin_SRC example
 total | started
-------+---------
  7514 |    7019
(1 row)

#+end_SRC

** job_gcs_output
#+NAME: Define job_gcs_output table
#+begin_src sql-mode :results silent
create table job_gcs_output(
id bigint generated by default as identity,
size bigint,
time timestamp,
path text unique
);

comment on table job_gcs_output is 'the path and size of resulting files from a test run';
comment on column job_gcs_output.size is 'size of the file in bytes';
comment on column job_gcs_output.time is 'time file was added';
comment on column job_gcs_output.path is 'full filepath, should be unique';
comment on column job_gcs_output.job_id is 'reference to the job in successful_jobs table';

create unique index output_path_idx on job_gcs_output(path);
#+end_src

** recent_jobs
A combined materialized view of job results and output, based on successful jobs in the last 30 days

#+NAME: Define recent_jobs
#+begin_src sql-mode
begin;
create materialized view recent_jobs as
select
  s.id as job_id,
  j.path as output_path,
  substring(substring(j.path, '\/[a-z\-_:\.\$ ()A-Z0-9]+$'),2) as file
  from      job_gcs_output j
            join successful_jobs s on (j.path like s.path||'%')
 where s.started >= current_timestamp - interval '30 days';

comment on materialized view recent_jobs is 'the file paths for all output of all successful jobs from the last 30 days';
comment on column recent_jobs.job_id is 'matches to successful_jobs.id';
comment on column recent_jobs.output_path is 'matches to job_gcs_output.path';
comment on column recent_jobs.file is 'the resulting filename at the end of the output_path';
commit;
#+end_src

** file
#+NAME: Define file
#+begin_src sql-mode :results silent
begin;
create unlogged table file(
  job_id bigint,
  filename text,
  path text,
  text_content text,
  json jsonb
);

comment on table file is 'the contents of job_gcs_output files';
comment on column file.job_id is 'the row id of the successful job this file is a part of';
comment on column file.path is 'the job_gcs_output path';
comment on column file.text_content is 'if file not json, the full content as text, else null.';
comment on column file.json is 'if json, the full content as json, else null';

commit;
#+end_src

* Postgres functions for loading data
** Load Successful Jobs
#+NAME: fn load_successful_jobs
#+begin_src sql-mode :results silent
create procedure load_successful_jobs(file text) as
  $$
begin
  create temporary table success_import(data jsonb) on commit drop;
execute format('copy success_import(data)
                from %L csv quote e''\x01'' delimiter e''\x02'';', $1); -- $1 is our file parameter

insert into successful_jobs (job,path,number,started)
select  i.data->>'job',
        i.data->>'path',
        i.data->>'number',
        to_timestamp(i.data->>'started', 'YYYY-MM-DD HH24:MI:SS') as started
  from success_import i;
raise notice '%', (SELECT array_to_json(array_agg(t)) FROM (select count(*) jobs_loaded from successful_jobs)t);
end;
$$ language plpgsql;

comment on procedure load_successful_jobs is 'load json file on postgres machine into successful_jobs';
#+end_src
** Load job_gcs_output
#+NAME: define load_job_gcs_output
#+begin_src sql-mode :results silent
create procedure load_job_gcs_output(file text) as
  $$
begin
    create temporary table job_import(size text, time text, path text) on commit drop;

    execute format('copy job_import(size,time,path) from %L csv;', $1);

    insert into job_gcs_output(size,time,path)

    select i.size::bigint as size,
        i.time::timestamp,
        i.path
    from job_import i;

raise notice '%', (select array_to_json(array_agg(t)) from (select count(*) as output_rows_added from job_gcs_output)t);
  end;
  $$ language plpgsql;

comment on procedure load_job_gcs_output is 'load csv on postgres server into job_gcs_output';
#+end_src

** Fetch file
isolated python function for fetching from the internet and downloading to our workspace folder
the resultiong file's name will be the hash of the job_gcs_output path, so we can ensure it is short, consistent, and unique.
we return this filename in the fucntion, so we can use it to copy from this file later.

#+NAME: Define fetch_file
#+begin_src sql-mode :results silent
begin;

create function fetch_file(filepath text) returns text
as $$
    import urllib.request
    import hashlib

    workdir = '/workspace/'

    public_prefix = 'https://gcsweb.k8s.io/gcs/';
    path = filepath.replace("gs://","")
    link = public_prefix + path
    output = ""

    with urllib.request.urlopen(link) as response:

     # create a hash for the resulting filename
     hash_object = hashlib.sha256(filepath.encode('utf-8'))
     hex_dig = hash_object.hexdigest()
     output = workdir + str(hex_dig) + '.txt'

     # read the file
     file = response.read().decode('utf-8')
     f = open(output, 'w')
     f.write(file)
     f.close()
    return output
    end;
$$ language plpython3u;

comment on function fetch_file is 'fetches given job_gcs_output path and fetches it via its gcsweb public link';
commit;
#+end_src


** Insert file
this is not complete yet, as i want to add json for json and text for text, but
the newlines in the file are creating a problem. which means we'll need to
change the fetch file function to strip newlines in json. I am uncertaion if we
should strip newlines in general though, or the best way to do that in python.

,#+begin_src sql-mode
drop procedure insert_file;
#+end_src

#+RESULTS:
#+begin_SRC example
DROP PROCEDURE
#+end_SRC

#+NAME: Define insert_file
#+begin_src sql-mode
begin;
create procedure insert_file(filepath text)
as $$
  declare
  blob_path text;
  copy_command text;
begin

  blob_path = fetch_file(filepath);

  insert into file(job_id, path)

  select j.job_id,
         j.output_path as path
    from recent_jobs j
   where j.output_path = filepath;

   raise notice '%',blob_path;
   copy_command = 'copy file(text_content) from '''||blob_path||''' where file.path = '''||filepath||''';';

  execute copy_command;
end;
$$
language plpgsql;

comment on  procedure insert_file is 'takes a file on sql path and inserts its content into file table for matching job_gcs_output file';

commit;
#+end_src

#+RESULTS: Define insert_file
#+begin_SRC example
BEGIN
postgres=*# postgres-*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres$*# postgres-*# CREATE PROCEDURE
postgres=*# postgres=*# COMMENT
postgres=*# postgres=*# COMMIT
#+end_SRC

** Insert Files for filename
takes a filename and inserts all files that match that filename
#+NAME: insert files for filename
#+begin_src sql-mode
begin;

create procedure insert_files_for_filename(filename text)
as $$
  declare
  table_record RECORD;
begin
  for table_record in (select * from recent_jobs where recent_jobs.file like '%'||filename)
    loop
      call insert_file(table_record.output_path);
    end loop;
end;
$$ language plpgsql;
commit;
#+end_src

* Load up the database

#+begin_src sql-mode
call load_successful_jobs('/workspace/latest_successful_jobs.json');
#+end_src

#+RESULTS:
#+begin_SRC example
NOTICE:  [{"jobs_loaded":7514}]
CALL
#+end_SRC

#+NAME: load up the job_gcs_output
#+begin_src sql-mode
call load_job_gcs_output('/workspace/massaged.csv');
#+end_src

#+RESULTS: load up the job_gcs_output
#+begin_SRC example
NOTICE:  [{"output_rows_added":435919}]
CALL
#+end_SRC

#+NAME: refresh recent jobs
#+begin_src sql-mode
refresh materialized view recent_jobs;
#+end_src

#+RESULTS: refresh recent jobs
#+begin_SRC example
REFRESH MATERIALIZED VIEW
#+end_SRC

* Current progress
** TODO get the files to actually upload
I can verify they are being downloaded, being found, being matched to the right
column in the right table, but I cannot verify that the files are actually being
uploaded into our file table. Why? What am i missing?
* scratch
* Appendix
** sql: list all temporary tables
From [[https://www.dbrnd.com/2017/06/postgresql-find-a-list-of-active-temp-tables-with-size-and-user-information-idle-connection/][Anvesh Patel's blog]]

I am using this to make sure the temporary tables we create duriong the copy are removed after.

#+NAME: list temporary tables
#+begin_src sql-mode
  SELECT
	n.nspname as SchemaName
	,c.relname as RelationName
	,CASE c.relkind
	WHEN 'r' THEN 'table'
	WHEN 'v' THEN 'view'
	WHEN 'i' THEN 'index'
	WHEN 'S' THEN 'sequence'
	WHEN 's' THEN 'special'
	END as RelationType
	,pg_catalog.pg_get_userbyid(c.relowner) as RelationOwner
	,pg_size_pretty(pg_relation_size(n.nspname ||'.'|| c.relname)) as RelationSize
FROM pg_catalog.pg_class c
LEFT JOIN pg_catalog.pg_namespace n
                ON n.oid = c.relnamespace
WHERE  c.relkind IN ('r','s')
AND  (n.nspname !~ '^pg_toast' and nspname like 'pg_temp%')
ORDER BY pg_relation_size(n.nspname ||'.'|| c.relname) DESC;
#+end_src

#+RESULTS: list temporary tables
#+begin_SRC example
 schemaname | relationname | relationtype | relationowner | relationsize
------------+--------------+--------------+---------------+--------------
(0 rows)

#+end_SRC

** current size

#+begin_src sql-mode
select (sum(size)/1073741824) as size_in_gb -- there are 1,073,741,824 bytes in a gB
  from recent_jobs
         join job_gcs_output on (recent_jobs.output_path = job_gcs_output.path)
 where file like '%json' or file like '%log';
#+end_src

#+RESULTS:
#+begin_SRC example
      size_in_gb
----------------------
 113.0309726139530540
(1 row)

#+end_SRC
